{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In3gth4aNYDM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers-interpret\n",
        "!pip install lime\n",
        "!pip install shap\n",
        "!pip install stop-words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import words\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from stop_words import get_stop_words\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "VuV7lksUijMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import os\n",
        "import zipfile\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import (\n",
        "    DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    RobertaForSequenceClassification\n",
        ")\n",
        "\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from transformers_interpret import SequenceClassificationExplainer\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from IPython.core.display import display, HTML\n",
        "import gc\n",
        "from collections import defaultdict\n",
        "\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "GzG0VXEqiltI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Phishing_Email.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7KjtEYHhip-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Label']=df['Email Type'].apply(lambda x: 1 if x=='Phishing Email' else 0)\n",
        "df = df.drop('Unnamed: 0', axis=1)\n",
        "df['Email Type'].value_counts()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-TtUQoD_izjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Distribution**"
      ],
      "metadata": {
        "id": "La1C3J69kjMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "email_type_counts = df['Email Type'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "colors = ['#66c2a5' if label == 'Safe Email' else '#fc8d62' for label in email_type_counts.index]\n",
        "\n",
        "plt.pie(email_type_counts, labels=email_type_counts.index,\n",
        "        autopct=lambda p: f'{p:.1f}% ({int(p * sum(email_type_counts) / 100)})',\n",
        "        textprops={'fontsize': 12, 'fontweight': 'bold'}, colors=colors)\n",
        "\n",
        "plt.title('Email Type Distribution', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.savefig('email_type_pie_chart.jpg', dpi=600)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q2zxeKe-i5Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove rows with null values**"
      ],
      "metadata": {
        "id": "vc8BbDsSkUln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if null values exist\n",
        "print(df.isnull().any())\n",
        "print('-------------------------------')\n",
        "print(df.isnull().sum())\n",
        "\n",
        "if df.isnull().values.any():\n",
        "    df = df.dropna()\n",
        "    print('After removing null values:')\n",
        "    print(df.isnull().any())\n",
        "    print('-------------------------------')\n",
        "    print(df.isnull().sum())\n",
        "else:\n",
        "    print('No null values found.')\n",
        "\n",
        "print(\"New Data length: \", len(df))\n",
        "\n",
        "df['Email Type'].value_counts()"
      ],
      "metadata": {
        "id": "qH0tn8chjC6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "A5SXU4Mjkcge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.replace('\\r', ' ')\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    return clean_text(text)\n",
        "\n",
        "df['Email_Text'] = df['Email Text'].apply(preprocess_text)\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "pmtXE_WOjHzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df[['Email_Text', 'Email Type', 'Label']]\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "fSyJs7T_jiNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Synonym Replacement Function**"
      ],
      "metadata": {
        "id": "dqXHJNOqu2fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def synonym_replacement(text, replace_ratio=0.2):\n",
        "    words = text.split()\n",
        "    new_words = words[:]\n",
        "\n",
        "    # Identify indices of non-stopwords\n",
        "    non_stop_indices = [i for i, word in enumerate(words) if word.lower() not in stop_words]\n",
        "\n",
        "    # Ensure at least one replacement for short texts\n",
        "    num_replacements = max(1, int(len(non_stop_indices) * replace_ratio))\n",
        "\n",
        "    if not non_stop_indices:\n",
        "        return text\n",
        "\n",
        "    indices_to_replace = random.sample(non_stop_indices, min(num_replacements, len(non_stop_indices)))\n",
        "\n",
        "    for idx in indices_to_replace:\n",
        "        word = words[idx]\n",
        "        synonyms = wordnet.synsets(word)\n",
        "\n",
        "        valid_synonyms = [\n",
        "            lemma.name().replace('_', ' ')\n",
        "            for syn in synonyms\n",
        "            for lemma in syn.lemmas()\n",
        "            if lemma.name().lower() != word.lower()\n",
        "        ]\n",
        "\n",
        "        # Replace with a random synonym if available\n",
        "        if valid_synonyms:\n",
        "            synonym = random.choice(valid_synonyms)\n",
        "            new_words[idx] = synonym\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def augment_phishing_data(email_text, replace_ratio=0.2):\n",
        "    return synonym_replacement(email_text, replace_ratio)"
      ],
      "metadata": {
        "id": "s3gYSjUrjpSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split dataset**"
      ],
      "metadata": {
        "id": "h-1cRUV6vJYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
        "#     data['Email_Text'],\n",
        "#     data['Email Type'],\n",
        "#     test_size=0.2,\n",
        "#     random_state=42,\n",
        "#     stratify=data['Email Type']\n",
        "# )\n",
        "\n",
        "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
        "    data['Email_Text'],\n",
        "    data['Email Type'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "PkOajzW4jrRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After initial split:\")\n",
        "print(f\"Train+Val size: {len(train_val_texts)}\")\n",
        "print(f\"Test size: {len(test_texts)}\")\n",
        "print(\"\\nTest set distribution:\")\n",
        "print(test_labels.value_counts())"
      ],
      "metadata": {
        "id": "woJaPMwRrP_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "#     train_val_texts,\n",
        "#     train_val_labels,\n",
        "#     test_size=0.2,\n",
        "#     random_state=42,\n",
        "#     stratify=train_val_labels\n",
        "# )\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_val_texts,\n",
        "    train_val_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Y_8FYD_-rTGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After train-val split:\")\n",
        "print(f\"Train size: {len(train_texts)}\")\n",
        "print(f\"Validation size: {len(val_texts)}\")\n",
        "print(f\"Test size: {len(test_texts)}\")\n",
        "\n",
        "print(\"\\nClass distribution before augmentation:\")\n",
        "print(\"Training set:\")\n",
        "print(train_labels.value_counts())\n",
        "print(\"\\nValidation set:\")\n",
        "print(val_labels.value_counts())\n",
        "print(\"\\nTest set:\")\n",
        "print(test_labels.value_counts())"
      ],
      "metadata": {
        "id": "0Jb8BekurZlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create balanced training set**"
      ],
      "metadata": {
        "id": "VLQ2Qd7JvX8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame({\n",
        "    'Email_Text': train_texts.reset_index(drop=True),\n",
        "    'Email Type': train_labels.reset_index(drop=True)\n",
        "})\n",
        "\n",
        "phishing_train = train_df[train_df['Email Type'] == 'Phishing Email'].copy()\n",
        "non_phishing_train = train_df[train_df['Email Type'] == 'Safe Email'].copy()\n",
        "\n",
        "majority_count = len(non_phishing_train)\n",
        "minority_count = len(phishing_train)\n",
        "samples_to_generate = majority_count - minority_count\n",
        "\n",
        "print(f\"Phishing emails in training: {minority_count}\")\n",
        "print(f\"Safe emails in training: {majority_count}\")\n",
        "print(f\"Samples to generate: {samples_to_generate}\")\n",
        "\n",
        "augmented_samples = []\n",
        "\n",
        "if samples_to_generate > 0:\n",
        "    while len(augmented_samples) < samples_to_generate:\n",
        "        for index, row in phishing_train.iterrows():\n",
        "            email_text = row['Email_Text']\n",
        "            augmented_text = augment_phishing_data(email_text)\n",
        "            augmented_samples.append({\n",
        "                'Email_Text': augmented_text,\n",
        "                'Email Type': row['Email Type'],\n",
        "            })\n",
        "            if len(augmented_samples) >= samples_to_generate:\n",
        "                break\n",
        "\n",
        "    augmented_df = pd.DataFrame(augmented_samples)\n",
        "    balanced_train_df = pd.concat([train_df, augmented_df], ignore_index=True)\n",
        "    balanced_train_df = balanced_train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "else:\n",
        "    balanced_train_df = train_df  # Already balanced\n",
        "\n",
        "print(\"\\nAfter augmentation:\")\n",
        "print(\"Balanced training set distribution:\")\n",
        "print(balanced_train_df['Email Type'].value_counts())\n",
        "print(f\"Final training set size: {len(balanced_train_df)}\")"
      ],
      "metadata": {
        "id": "iel_uzNFryCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_train_texts = balanced_train_df['Email_Text']\n",
        "final_train_labels = balanced_train_df['Email Type']\n",
        "\n",
        "\n",
        "val_texts = val_texts.reset_index(drop=True)\n",
        "val_labels = val_labels.reset_index(drop=True)\n",
        "test_texts = test_texts.reset_index(drop=True)\n",
        "test_labels = test_labels.reset_index(drop=True)\n",
        "\n",
        "print(\"Final dataset sizes:\")\n",
        "print(f\"Training: {len(final_train_texts)}\")\n",
        "print(f\"Validation: {len(val_texts)}\")\n",
        "print(f\"Test: {len(test_texts)}\")"
      ],
      "metadata": {
        "id": "q8C0fyjcsA51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load roberta-base Tokenizer and Model**"
      ],
      "metadata": {
        "id": "CYzjQiPtviBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)"
      ],
      "metadata": {
        "id": "WGjFoL75sOdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizing data**"
      ],
      "metadata": {
        "id": "KbSF86M_wGoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "train_encodings = tokenizer(final_train_texts.tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n",
        "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n",
        "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=MAX_LEN)"
      ],
      "metadata": {
        "id": "3dpU9I05sQq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Creating PyTorch datasets***"
      ],
      "metadata": {
        "id": "mO8NxLhiwRZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(\n",
        "    torch.tensor(train_encodings['input_ids']),\n",
        "    torch.tensor(train_encodings['attention_mask']),\n",
        "    torch.tensor(final_train_labels.map({'Phishing Email': 1, 'Safe Email': 0}).tolist())\n",
        ")\n",
        "val_dataset = TensorDataset(\n",
        "    torch.tensor(val_encodings['input_ids']),\n",
        "    torch.tensor(val_encodings['attention_mask']),\n",
        "    torch.tensor(val_labels.map({'Phishing Email': 1, 'Safe Email': 0}).tolist())\n",
        ")\n",
        "test_dataset = TensorDataset(\n",
        "    torch.tensor(test_encodings['input_ids']),\n",
        "    torch.tensor(test_encodings['attention_mask']),\n",
        "    torch.tensor(test_labels.map({'Phishing Email': 1, 'Safe Email': 0}).tolist())\n",
        ")"
      ],
      "metadata": {
        "id": "aFJHNa2WsWId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Create DataLoaders***"
      ],
      "metadata": {
        "id": "--tg6xwQwZzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "fWW9AMuJsk8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "U_jmG66lss0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model training**"
      ],
      "metadata": {
        "id": "YtiOEYznzezN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_accuracy = correct / total_samples * 100\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}] - Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total_samples = 0\n",
        "    val_total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_loader:\n",
        "            val_input_ids = val_batch[0].to(device)\n",
        "            val_attention_mask = val_batch[1].to(device)\n",
        "            val_label = val_batch[2].to(device)\n",
        "\n",
        "            val_outputs = model(val_input_ids, attention_mask=val_attention_mask, labels=val_label)\n",
        "            val_loss = val_outputs.loss\n",
        "            val_total_loss += val_loss.item()\n",
        "\n",
        "            val_logits = val_outputs.logits\n",
        "            val_predictions = torch.argmax(val_logits, dim=1)\n",
        "            val_correct += (val_predictions == val_label).sum().item()\n",
        "            val_total_samples += val_label.size(0)\n",
        "\n",
        "    val_epoch_accuracy = val_correct / val_total_samples * 100\n",
        "    val_epoch_loss = val_total_loss / len(val_loader)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "    val_accuracies.append(val_epoch_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}] - Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # Testing phase\n",
        "    test_correct = 0\n",
        "    test_total_samples = 0\n",
        "    test_total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for test_batch in test_loader:\n",
        "            test_input_ids = test_batch[0].to(device)\n",
        "            test_attention_mask = test_batch[1].to(device)\n",
        "            test_label = test_batch[2].to(device)\n",
        "\n",
        "            test_outputs = model(test_input_ids, attention_mask=test_attention_mask, labels=test_label)\n",
        "            test_loss = test_outputs.loss\n",
        "            test_total_loss += test_loss.item()\n",
        "\n",
        "            test_logits = test_outputs.logits\n",
        "            test_predictions = torch.argmax(test_logits, dim=1)\n",
        "            test_correct += (test_predictions == test_label).sum().item()\n",
        "            test_total_samples += test_label.size(0)\n",
        "\n",
        "    test_accuracy = test_correct / test_total_samples * 100\n",
        "    test_loss = test_total_loss / len(test_loader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}] - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print(\"\\n\" + \"-\"*70)"
      ],
      "metadata": {
        "id": "6ZLhcYvVsvIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Accuracy plots\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.plot(range(1, epochs + 1), test_accuracies, label='Test Accuracy', linestyle='--')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Loss plots\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.plot(range(1, epochs + 1), test_losses, label='Test Loss', linestyle='--')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss_and_accuracy.jpg', dpi=600)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2F3E29RYs01s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation Report**"
      ],
      "metadata": {
        "id": "Pw--QFF4wm0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_mask = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    predictions.extend(torch.argmax(logits, dim=1).cpu().detach().numpy())\n",
        "    true_labels.extend(labels.cpu().detach().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Gn8LsSmHs3f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "class_report = classification_report(true_labels, predictions)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "U9V-M4b3s5vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "class_names=['Safe Email', 'Phishing Email']\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "plt.figure(figsize=(5, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "plt.savefig('Confusion',dpi=600);"
      ],
      "metadata": {
        "id": "kcY7qqj6s7zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        probs = torch.softmax(outputs.logits, dim=1)  # Convert logits to probabilities\n",
        "\n",
        "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n"
      ],
      "metadata": {
        "id": "oL-OoN20s-ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC curve and AUC**"
      ],
      "metadata": {
        "id": "rs-7QkPpw19p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.3f}')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "# plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "roc_save_path = 'roc_auc_curve.jpg'\n",
        "plt.savefig(roc_save_path, dpi=600)\n",
        "plt.show()\n",
        "\n",
        "print(f\"AUC Score: {roc_auc_score(all_labels, all_probs):.3f}\")"
      ],
      "metadata": {
        "id": "LlgfDPhmtARh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explainability Analysis**"
      ],
      "metadata": {
        "id": "wBugaFEIyzhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label = {0: 'Safe Email', 1: 'Phishing Email'}\n",
        "model.config.label2id = {'Safe Email': 0, 'Phishing Email': 1}"
      ],
      "metadata": {
        "id": "k84KdorrtFdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SELECT TEXT TO INTERPRET"
      ],
      "metadata": {
        "id": "pxizxv74tgmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_index = 3650\n",
        "text_to_interpret = test_texts.iloc[text_index]\n",
        "selected_text_label = test_labels.iloc[text_index]\n",
        "print(f\"Text to Interpret: {text_to_interpret}\")\n",
        "print(f\"Selected Text Label: {selected_text_label}\")"
      ],
      "metadata": {
        "id": "cwbSN12ztYcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFORMERS INTERPRET"
      ],
      "metadata": {
        "id": "ueDsQlF_tmjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret_text(text, label, model, tokenizer):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Actual Class: {label}\")\n",
        "\n",
        "    cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
        "\n",
        "    word_attributions = cls_explainer(text)\n",
        "    print(\"Word Attributions:\", word_attributions)\n",
        "\n",
        "    visualization_path = 'roberta_viz_1.html'\n",
        "    cls_explainer.visualize(visualization_path)\n",
        "\n",
        "    return word_attributions\n",
        "\n",
        "attributions = interpret_text(text_to_interpret, selected_text_label, model, tokenizer)"
      ],
      "metadata": {
        "id": "nSRVMPI0tkjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIME EXPLANATION"
      ],
      "metadata": {
        "id": "qzFobV5ytwYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = ''\n",
        "class_names = ['Legitimate Email', 'Phishing Email']\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "def explain_prediction(text):\n",
        "    def model_predict(texts):\n",
        "        input_encodings = tokenizer(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors='pt')\n",
        "        input_ids = input_encodings['input_ids'].to(device)\n",
        "        attention_mask = input_encodings['attention_mask'].to(device)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "        return logits.cpu().numpy()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    explanation = explainer.explain_instance(text, model_predict, num_features=15, top_labels=1)\n",
        "    predicted_probabilities = model_predict([text])\n",
        "    prediction_probabilities = {class_names[i]: round(float(predicted_probabilities[0][i]), 2) for i in range(len(class_names))}\n",
        "\n",
        "    print(\"Prediction probabilities:\")\n",
        "    print(prediction_probabilities)\n",
        "\n",
        "    exp_class = explanation.available_labels()[0]\n",
        "    exp = explanation.as_list(label=exp_class)\n",
        "    print(f\"Explanation for {exp_class}: {exp}\")\n",
        "\n",
        "    display(HTML(explanation.as_html()))\n",
        "\n",
        "    # Save HTML explanation\n",
        "    html_path = os.path.join(save_dir, 'lime_explanation_1.html')\n",
        "    with open(html_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(explanation.as_html())\n",
        "\n",
        "    # Save JPG plot\n",
        "    exp_plot = explanation.as_pyplot_figure(label=exp_class)\n",
        "    exp_plot.set_figwidth(10)\n",
        "    exp_plot.set_figheight(6)\n",
        "    jpg_path = os.path.join(save_dir, 'lime_explanation_1.jpg')\n",
        "    exp_plot.savefig(jpg_path, bbox_inches='tight', dpi=600, format='jpg')\n",
        "\n",
        "    return exp\n",
        "\n",
        "# Get LIME explanation\n",
        "lime = explain_prediction(text_to_interpret)\n",
        "# Extract LIME words\n",
        "lime_words = [word for word, score in lime]"
      ],
      "metadata": {
        "id": "XmWdS4aRtsGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROCESS TRANSFORMER ATTRIBUTIONS"
      ],
      "metadata": {
        "id": "Q7trROA5t71-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group attribution scores by token\n",
        "token_scores = defaultdict(list)\n",
        "for token, score in attributions:\n",
        "    token_scores[token].append(score)\n",
        "\n",
        "# Calculate average attribution per token\n",
        "avg_token_attributions = {\n",
        "    token: sum(scores) / len(scores)\n",
        "    for token, scores in token_scores.items()\n",
        "}\n",
        "\n",
        "# Create trans_values dictionary\n",
        "trans_values = {word: avg_token_attributions.get(word, 0.0) for word in lime_words}\n",
        "\n",
        "print(\"\\nTransformer Attribution Values:\")\n",
        "for word in lime_words:\n",
        "    value = trans_values[word]\n",
        "    print(f\"{word:<10} -> {value:.5f}\")"
      ],
      "metadata": {
        "id": "UNw7s2tgt6Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NORMALIZATION AND HYBRID SCORING (LITA)**"
      ],
      "metadata": {
        "id": "LHpoDYlAuFga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_to_minus1_plus1(values):\n",
        "    min_val = min(values)\n",
        "    max_val = max(values)\n",
        "    range_val = max_val - min_val\n",
        "    if range_val == 0:\n",
        "        return [0 for _ in values]\n",
        "    return [((x - min_val) / range_val) * 2 - 1 for x in values]\n",
        "\n",
        "# Extract lists of scores for lime and trans in the same order\n",
        "lime_scores = [score for _, score in lime]\n",
        "trans_scores = [trans_values[word] for word, _ in lime]\n",
        "\n",
        "# Normalize scores\n",
        "lime_norm = normalize_to_minus1_plus1(lime_scores)\n",
        "trans_norm = normalize_to_minus1_plus1(trans_scores)\n",
        "\n",
        "# Alpha weight\n",
        "alpha = 0.5\n",
        "\n",
        "# Calculate hybrid scores\n",
        "hybrid_scores = [\n",
        "    (lime[i][0], lime_scores[i], lime_norm[i], trans_scores[i], trans_norm[i], alpha * lime_norm[i] + (1 - alpha) * trans_norm[i])\n",
        "    for i in range(len(lime))\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Word':<12} {'LIME Score':>10} {'LIME Norm':>10} {'Trans Score':>12} {'Trans Norm':>12} {'Hybrid Score':>14}\")\n",
        "print('-'*80)\n",
        "\n",
        "for word, lime_s, lime_n, trans_s, trans_n, hybrid in hybrid_scores:\n",
        "    print(f\"{word:<12} {lime_s:10.5f} {lime_n:10.5f} {trans_s:12.5f} {trans_n:12.5f} {hybrid:14.5f}\")"
      ],
      "metadata": {
        "id": "JQboFGbluCnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_dict = {word: hybrid for word, _, _, _, _, hybrid in hybrid_scores}\n",
        "\n",
        "print(\"\\nHybrid Scores Dictionary:\")\n",
        "for word, score in hybrid_dict.items():\n",
        "    print(f\"'{word}': {score:.5f},\")"
      ],
      "metadata": {
        "id": "EbIRRzEKuS70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "words = lime_words\n",
        "hybrid_scores = [hybrid_dict[word] for word in words]\n",
        "\n",
        "colors = ['green' if score >= 0 else 'red' for score in hybrid_scores]\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "bars = plt.bar(words, hybrid_scores, color=colors)\n",
        "\n",
        "plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
        "plt.title(\"Hybrid Scores (Î±=0.5)\", pad=20, fontweight='bold', fontsize=18)\n",
        "plt.ylabel(\"Normalized Score [-1, 1]\", labelpad=10, fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=13, fontweight='bold')\n",
        "plt.yticks(np.arange(-1, 1.25, 0.25), fontsize=12)\n",
        "plt.ylim(-1, 1)\n",
        "plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"hybrid_phishing_scores2.jpg\", dpi=300, format='jpg')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3gNSt__xuX-a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}